---
title: Perceptron
category:
  - ü§ñ ML
subcategory:
  - Supervisado
  - Clasificacion
tags:
  - ML
  - Supervisado
  - Red_Neuronal
status:
  - ‚úÖ Completado
date: 2024-11-08
asigned_to:
  - "[[Armando]]"
---
--- 
# üìù Ficha T√©cnica: Perceptron

## ‚úÖ Estado de Compleci√≥n
- [x] **T√≠tulo del algoritmo**
- [x] **Representaci√≥n gr√°fica del algoritmo**
- [x] **Introducci√≥n al algoritmo y su relevancia**
- [x] **Bases Matem√°ticas del Algoritmo**
- [x] **C√≥digo de ejemplo en Python**
- [x] **Descripci√≥n de los tipos de datos aplicables**
- [x] **Supuestos de los datos**
- [x] **Ejemplos de aplicaci√≥n pr√°ctica**
- [x] **Enlaces a recursos adicionales para profundizar en el tema**
- [x] **Referencias**

---
## 1. T√≠tulo del Algoritmo

### **Perceptr√≥n**

---
## 2. Representaci√≥n Gr√°fica del Algoritmo

![[perceptron_icon.png]]

Referencia: <a href="https://www.flaticon.com/free-icons/perceptron" title="perceptron icons">Perceptron icons created by orvipixel - Flaticon</a>

---
## 3. Introducci√≥n al Algoritmo 

El **perceptr√≥n** es uno de los modelos fundamentales en el campo de las [[Redes_Neuronales_Artificiales]], el cual fue introducido por Frank Rosenblatt en los a√±os cincuentas en su famoso paper *(Rosenblatt, 1958)*. Esta estructura est√° inspirada en el funcionamiento de una neurona biol√≥gica, procesando entradas mediante pesos asignados y produciendo una **salida binaria**. Estructuralmente, consiste en una **√∫nica capa** de nodos (neurona o TLU) que aplican una funci√≥n de activaci√≥n, para clasificar datos en categor√≠as distintas. Aunque su capacidad est√° limitada a resolver problemas **linealmente separables**, el perceptr√≥n sent√≥ las bases para el desarrollo de arquitecturas m√°s complejas, como los perceptrones multicapa. A pesar de sus limitaciones, el perceptr√≥n es una herramienta educativa e imprescindible en los recursos de ense√±anza del aprendizaje autom√°tico y profundo *(Du et al., 2022; Li, 2024; Shanthamallu et al., n.d.)*.

---
## 4. Bases Matem√°ticas del Algoritmo

El **algoritmo del perceptr√≥n** comienza inicializando el vector de pesos $\mathbf{w} = [w_1, w_2, \dots, w_n]$ y el t√©rmino de sesgo $b$ (tambi√©n conocido como *bias*) con valores aleatorios. Para cada muestra de entrenamiento con el vector de entrada de caracter√≠sticas $\mathbf{x} = [x_1, x_2, \dots, x_n]$ y la salida deseada $d$, el algoritmo calcula una suma ponderada $z$ como:

$$
z (\mathbf{x}) = \mathbf{w} \cdot \mathbf{x} + b = \sum_{i=1}^{n} w_i x_i + b
$$

Esta suma se pasa a trav√©s de la funci√≥n de activaci√≥n $a = a(z)$ para producir la salida predicha $\hat{y}$. T√≠picamente, la funci√≥n de activaci√≥n usada es la **funci√≥n escal√≥n unitaria**:

$$
\hat{y} = a(z) =
\begin{cases}
1 & \text{si } z > 0 \\
0 & \text{si} \; z \leq 0
\end{cases}
$$

El error se calcula restando la salida predicha de la salida deseada:

$$
\text{Error} = d - \hat{y}
$$

Para minimizar este error, los pesos y el sesgo se actualizan utilizando la tasa de aprendizaje $\eta$:

$$
w_i \leftarrow w_i + \eta \times \text{Error} \times x_i \quad \forall i = 1, 2, \dots, n
$$

$$
b \leftarrow b + \eta \times \text{Error}
$$

Este proceso se repite iterativamente para cada ejemplo de entrenamiento y a lo largo de m√∫ltiples √©pocas hasta que los pesos converjan (es decir, el error sea m√≠nimo) o se alcance un n√∫mero predefinido de iteraciones. A trav√©s de estas actualizaciones, el perceptr√≥n ajusta su l√≠nea de decisi√≥n para clasificar mejor los datos de entrada *(Du et al., 2022; Li, 2024; Shanthamallu et al., n.d.)*.

### 4.1 Ilustraci√≥n del funcionamiento (*Opcional*)

![[perceptron_des.png]]

**Referencia:** Imagen tomada de *(Shanthamallu et al., n.d.)*

---
## 5. C√≥digo de Ejemplo en Python

Se puede acceder al ejemplo pr√°ctico b√°sico de implementaci√≥n en:

[Clasificador de im√°genes](C:\Users\arhui\Documents\projects\TAIA\src\basic_code\Perceptron.ipynb)

---
## 6.  Tipos de Datos

- Esencialmente cualquier tipo de dato siempre que exista una **representaci√≥n num√©rica**, ya sea continua o discreta.  

---
## 7.  Supuestos de los datos

- **Separabilidad lineal**
	-  Debe existir un hiperplano que pueda separar perfectamente las dos clases. 
- **Input num√©rico**
	-  Las caracter√≠sticas de entrada $x_i$ deben ser num√©ricas. 

--- 
## 8. Ejemplos de Aplicaci√≥n

- **Clasificaci√≥n de Spam en correos electr√≥nicos**: 
	- El perceptr√≥n puede clasificar mensajes como "spam" o "no spam" analizando caracter√≠sticas como palabras clave, remitente y frecuencia de ciertos t√©rminos.
- **Clasificaci√≥n de im√°genes m√©dicas b√°sico**:
	- Ayuda a distinguir entre im√°genes de tejidos sanos y an√≥malos, facilitando el diagn√≥stico temprano de enfermedades.
---
## 9. Recursos Adicionales

- [Gu√≠a completa del perceptr√≥n](https://pabloinsente.github.io/the-perceptron)
- [Simulador de redes neuronales](https://playground.tensorflow.org/#activation=sigmoid&batchSize=7&dataset=gauss&regDataset=reg-plane&learningRate=0.003&regularizationRate=0&noise=25&networkShape=1,2&seed=0.14476&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false) 
	- Puede ser utilizado para simular un perceptr√≥n.
---
## 10. Referencias

1. Du, Ke-Lin, Chi-Sing Leung, Wai Ho Mow, y M. N. S. Swamy. ¬´Perceptron: Learning, Generalization, Model Selection, Fault Tolerance, and Role in the Deep Learning Era¬ª. _Mathematics_ 10, n.o 24 (13 de diciembre de 2022): 4730. [https://doi.org/10.3390/math10244730](https://doi.org/10.3390/math10244730).
2. Li, Hang. _Machine Learning Methods_. Singapore: Springer Nature Singapore, 2024. [https://doi.org/10.1007/978-981-99-3917-6](https://doi.org/10.1007/978-981-99-3917-6).
3. Rosenblatt, F. ¬´The Perceptron: A Probabilistic Model for Information Storage and Organization in the Brain.¬ª _Psychological Review_ 65, n.o 6 (1958): 386-408. [https://doi.org/10.1037/h0042519](https://doi.org/10.1037/h0042519).
4. Shanthamallu, Uday Shankar, Andreas Spanias, Khalid Sayood, Kenichi Kanatani, Nasser Kehtarnavaz, Abhishek Sehgal, Shane Parris, y Arian Azarang. _Machine and Deep Learning Algorithms and Applications_, s.¬†f.