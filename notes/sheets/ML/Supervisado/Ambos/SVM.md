---
title: SVM
category:
  - ü§ñ ML
subcategory:
  - Supervisado
  - Clasificacion
  - Regresion
tags:
  - kernel
  - margin
  - support_vectors
  - hyperplane
status:
  - üîµ En Progreso
date: 24_11_2024
asigned_to:
  - "[[Armando]]"
---
--- 
# üìù Ficha T√©cnica: SVM

## ‚úÖ Estado de Compleci√≥n
- [x] **T√≠tulo del algoritmo**
- [x] **Representaci√≥n gr√°fica del algoritmo**
- [x] **Introducci√≥n al algoritmo y su relevancia**
- [x] **Bases Matem√°ticas del Algoritmo**
- [ ] **C√≥digo de ejemplo en Python**
- [x] **Descripci√≥n de los tipos de datos aplicables**
- [x] **Supuestos de los datos**
- [x] **Ejemplos de aplicaci√≥n pr√°ctica**
- [x] **Enlaces a recursos adicionales para profundizar en el tema**
- [x] **Referencias**

---
## 1. T√≠tulo del Algoritmo

### **M√°quina de Vectores de Soporte *(SVM)***

---
## 2. Representaci√≥n Gr√°fica del Algoritmo

![[svm_icon.png]]

Referencia: http://qingkaikong.blogspot.com/2016/12/machine-learning-7-support-vector.html

---
## 3. Introducci√≥n al Algoritmo 

La **M√°quina de Vectores de Soporte *(SVM, por sus siglas en ingl√©s)*** se introdujo en los a√±os noventa por Cortes y Vapnik, como una herramienta fundamental en el campo del aprendizaje autom√°tico supervisado *(Cortes & Vapnik, 1995)*. Los algoritmos SVM han adquirido relevancia a lo largo de los a√±os por su robustez y flexibilidad en espacios de **alta dimensi√≥n** y de datos limitados, as√≠ como por su capacidad de manejo de datos **lineales** y **no lineales**, en tareas tanto de **clasificaci√≥n** como de **regresi√≥n** *(Cano Lengua & Papa Quiroz, 2020; Dabas, 2024)*. 

El funcionamiento de una SVM, se basa en la b√∫squeda de la frontera de separaci√≥n √≥ptima (**hiperplano**), que mejor separa los puntos de datos entre las clases. Esta frontera maximiza el **margen**, que es la distancia entre el hiperplano y los puntos m√°s cercanos de los datos de cada clase. Estos puntos m√°s cercanos se llaman **vectores de soporte** porque tienen la responsabilidad significativa de definir la posici√≥n y orientaci√≥n del hiperplano. Para el caso de datos no lineales, las SVM transforman el espacio de caracter√≠sticas original en uno de mayor dimensi√≥n donde es posible separar las clases linealmente, a partir de funciones **kernel**.  *(Cortes & Vapnik, 1995; Li, 2024)*. 

Hoy en d√≠a, las SVM se aplican en diferentes campos que van desde la clasificaci√≥n de im√°genes hasta la bioinform√°tica. Sin embargo, pese a que se mantienen latentes en la actualidad, su efectividad depende fuertemente de la elecci√≥n cuidadosa del kernel y de los hiperpar√°metros, adem√°s que pueden requerir gastos computacionales extensivos en aplicaciones a gran escala *(Cano Lengua & Papa Quiroz, 2020; Dabas, 2024)*. 

---
## 4. Bases Matem√°ticas del Algoritmo

El objetivo cl√°sico e hist√≥rico de las SVM, es encontrar un hiperplano que separa dos clases de datos con el **m√°ximo margen** posible. Matem√°ticamente, el hiperplano se define como: $$ \mathbf{w} \cdot \mathbf{x} + b = 0 $$
donde $\mathbf{w}$ es el vector normal al hiperplano y $b$ es el t√©rmino de sesgo. El margen $\gamma$ se calcula como: $$ \gamma = \frac{2}{\|\mathbf{w}\|} $$ Maximizar $\gamma$ equivale a minimizar $\|\mathbf{w}\|$. Para asegurar una clasificaci√≥n correcta, se imponen las restricciones: $$ y_i (\mathbf{w} \cdot \mathbf{x}_i + b) \geq 1, \quad \forall i $$
Este problema de optimizaci√≥n es cuadr√°tico y convexo. Mediante la **formulaci√≥n dual**, se introducen multiplicadores de Lagrange $\alpha_i$, permitiendo utilizar el **truco del n√∫cleo** (*kernel trick*) para manejar datos no linealmente separables: $$ K(\mathbf{x}_i, \mathbf{x}_j) = \phi(\mathbf{x}_i) \cdot \phi(\mathbf{x}_j) $$ Las SVM tambi√©n pueden manejar casos donde los datos no son perfectamente separables mediante el **margen suave**, introduciendo variables de holgura $\xi_i$ y un par√°metro de regularizaci√≥n $C$: $$ \min \left( \frac{1}{2} \|\mathbf{w}\|^2 + C \sum_{i=1}^m \xi_i \right) $$
**Referencias:**  *(Cano Lengua & Papa Quiroz, 2020; Li, 2024)*
### 4.1 Ilustraci√≥n del funcionamiento 

![[svm_des.png]]

**Referencia:** https://www.sciencedirect.com/science/article/pii/S2665963822000641

---
## 5. C√≥digo de Ejemplo en Python

 >**Instrucciones**: Incluir un bloque de c√≥digo funcional en Python que ilustre la implementaci√≥n b√°sica del algoritmo. Aseg√∫rese de que el c√≥digo est√© bien documentado.

```python
# Code example in Python

def code_exaple()
	pass
````

---
## 6.  Tipos de Datos

- Pueden manejar diversos tipos de datos, siempre que se **representen adecuadamente en forma de vectores de caracter√≠sticas num√©ricas**. Estos incluyen:
	-  Num√©ricos
	-  Categ√≥ricos
	-  Texto
	-  Im√°genes
	-  Series de tiempo
	- Multimodales
	- etc

---
## 7.  Supuestos de los datos

- **Separabilidad**
	-  Los datos son o pueden (mediante transformaciones) ser separables por un margen claro.
- **Escalado de caracter√≠sticas**
	-  Las SVM son sensibles a la escala de las caracter√≠sticas, por lo que es importante escalar 
- **Clases balanceadas**
	- No es un supuesto estricto, pero es recomendado. Un desbalance significativo en las clases puede sesgar el modelo hacia la clase mayoritaria. 


--- 
## 8. Ejemplos de Aplicaci√≥n

- [weiboanalysis](https://github.com/Zephery/weiboanalysis) es un proyecto que aplica SVM (junto con otros algoritmos) para el **An√°lisis de sentimientos** y **clasificaci√≥n de textos**.
- [Speech-Emotion-Recognition](https://github.com/Renovamen/Speech-Emotion-Recognition) es un proyecto que utiliza SVM (entre otras t√©cnicas) para el **reconocimiento de emociones en se√±ales de voz**.
- [HablaImaginada-DeepLearning](https://github.com/EdgarMoyete/HablaImaginada-DeepLearning) propone una arquitectura que combina CNN y LSTM para la clasificaci√≥n de se√±ales EEG de habla imaginada. Aunque se centra en redes neuronales, tambi√©n incluye implementaciones de SVM para comparaci√≥n. 

---
## 9. Recursos Adicionales

- [IBM -SVM](https://www.ibm.com/mx-es/topics/support-vector-machine?utm_source=chatgpt.com) explicaci√≥n detallada sobre qu√© es una SVM, tipos, funcionamiento y aplicaciones. 
-  [Aspectos matem√°ticos en M√°quinas de Vectores Soporte (SVM)](https://uvadoc.uva.es/handle/10324/63431?utm_source=chatgpt.com) trabajo de fin de m√°ster analiza detalladamente los aspectos matem√°ticos subyacentes a la metodolog√≠a SVM. 
- [SVM Playground](https://aelbaza.github.io/SVMPlayground/?utm_source=chatgpt.com#dataset=circle&noise=10&seed=0.92236&showTestData=false&discretize=false&percTrainData=80&parameterC=11.8&polyDegree=2&gamma=0.5) un simulador interactivo que permite experimentar con SVM directamente en el navegador, ajustando par√°metros como el tipo de kernel, el valor de C y el grado del polinomio.

---
## 10. Referencias

 1. Cano Lengua, Miguel Angel, y Erik Alex Papa Quiroz. ¬´A Systematic Literature Review on Support Vector Machines Applied to Classification¬ª. En _2020 IEEE Engineering International Research Conference (EIRCON)_, 1-4. Lima, Peru: IEEE, 2020. [https://doi.org/10.1109/EIRCON51178.2020.9254028](https://doi.org/10.1109/EIRCON51178.2020.9254028).

2. Cortes, Corinna, y Vladimir Vapnik. ¬´Support-Vector Networks¬ª. _Machine Learning_ 20, n.o 3 (septiembre de 1995): 273-97. [https://doi.org/10.1007/BF00994018](https://doi.org/10.1007/BF00994018).

3. Dabas, Ali. ¬´Application of Support Vector Machines in Machine Learning¬ª. Preprints, 2 de agosto de 2024. [https://doi.org/10.36227/techrxiv.172263291.17505159/v1](https://doi.org/10.36227/techrxiv.172263291.17505159/v1).

4. Li, Hang. _Machine Learning Methods_. Singapore: Springer Nature Singapore, 2024. [https://doi.org/10.1007/978-981-99-3917-6](https://doi.org/10.1007/978-981-99-3917-6).