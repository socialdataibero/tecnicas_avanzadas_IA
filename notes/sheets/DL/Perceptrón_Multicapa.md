---
title: Perceptr√≥n_Multicapa
category:
  - üß† DL
subcategory:
  - None
tags:
  - Etiqueta1
  - Etiqueta2
  - Etiqueta3
status:
  - üîµ En Progreso
date: 2024-11-12
asigned_to:
  - "[[Armando]]"
---
--- 
# üìù Ficha T√©cnica: Perceptr√≥n_Multicapa

## ‚úÖ Estado de Compleci√≥n
- [x] **T√≠tulo del algoritmo**
- [x] **Representaci√≥n gr√°fica del algoritmo**
- [x] **Introducci√≥n al algoritmo y su relevancia**
- [x] **Bases Matem√°ticas del Algoritmo**
- [ ] **C√≥digo de ejemplo en Python**
- [ ] **Descripci√≥n de los tipos de datos aplicables**
- [ ] **Supuestos de los datos**
- [ ] **Ejemplos de aplicaci√≥n pr√°ctica**
- [ ] **Enlaces a recursos adicionales para profundizar en el tema**
- [x] **Referencias**

---
## 1. T√≠tulo del Algoritmo

### **Perceptr√≥n multicapa**

---
## 2. Representaci√≥n Gr√°fica del Algoritmo

![[perceptron_multicapa_icon.png]]

Referencia: <a href="https://www.flaticon.com/free-icons/neural-network" title="neural network icons">Neural network icons created by Freepik - Flaticon</a> 

---
## 3. Introducci√≥n al Algoritmo 

El **perceptr√≥n multicapa** (MLP, por sus siglas en ingl√©s) es una de las arquitecturas de [[Redes_Neuronales_Artificiales]] m√°s utilizadas y populares dentro del aprendizaje profundo. Debe su nombre a su estructura basada en **m√∫ltiples capas** de neuronas, t√≠picamente una **capa de entrada** , dos o m√°s **capas ocultas** y una **capa de salida**. Los MLPs son capaces de modelar relaciones complejas y no lineales en los datos (a diferencia del [[Perceptron]] simple), lo que los convierte en herramientas poderosas en diversas tareas del aprendizaje autom√°tico, incluyendo, la regresi√≥n, la clasificaci√≥n y  el reconocimiento de patrones. Aunque son modelos muy potentes y vers√°tiles, estos pueden ser computacionalmente intensivos y propensos al sobreajuste (overfitting), lo que ha impulsado el desarrollo de arquitecturas m√°s avanzadas *(Basha et al., 2023; Elansari et al., 2024; Shanthamallu et al., n.d.)*.

---
## 4. Bases Matem√°ticas del Algoritmo

Matem√°ticamente, un MLP procesa la informaci√≥n mediante una serie de transformaciones lineales y no lineales. Para cada capa $l$, la entrada se multiplica por una matriz de pesos $W^{(l)}$ y se le a√±ade un vector de *bias*  $\mathbf{b}^{(l)}$, obteniendo as√≠ el vector:

$$
\mathbf{z}^{(l)} = W^{(l)} \mathbf{a}^{(l-1)} + \mathbf{b}^{(l)}
$$

donde $\mathbf{a}^{(l-1)}$ es la salida de la capa anterior. A continuaci√≥n, se aplica una funci√≥n de activaci√≥n $\phi^{(l)}$ de manera elemento a elemento para introducir no linealidad, resultando en:

$$
\mathbf{a}^{(l)} = \phi^{(l)}(\mathbf{z}^{(l)})
$$

Este proceso se repite a lo largo de todas las capas ocultas hasta llegar a la capa de salida, donde la activaci√≥n final representa la predicci√≥n del modelo:

$$
\hat{\mathbf{y}} = \mathbf{a}^{(L)}
$$

El entrenamiento del MLP se realiza mediante el algoritmo de **retropropagaci√≥n**, que ajusta los pesos y sesgos minimizando una funci√≥n de p√©rdida. Este ajuste se logra calculando los gradientes de la funci√≥n de p√©rdida respecto a cada par√°metro y actualiz√°ndolos utilizando m√©todos de optimizaci√≥n *(Basha et al., 2023; Elansari et al., 2024; Shanthamallu et al., n.d.)*.

### 4.1 Ilustraci√≥n del funcionamiento 

![[perceptron_multicapa_des.png]]

**Referencia:** https://link.springer.com/article/10.1007/s10115-023-01959-7#citeas 

---
## 5. C√≥digo de Ejemplo en Python

 >**Instrucciones**: Incluir un bloque de c√≥digo funcional en Python que ilustre la implementaci√≥n b√°sica del algoritmo. Aseg√∫rese de que el c√≥digo est√© bien documentado.

```python
# Code example in Python

def code_exaple()
	pass
````

---
## 6.  Tipos de Datos

>*Instrucciones:* Detallar los tipos de datos con los que el algoritmo trabaja mejor (por ejemplo, datos tabulares, series de tiempo, im√°genes, texto, etc.). Indique tambi√©n si requiere preprocesamiento de datos espec√≠fico.

- Tipo 1
- Tipo 2

---
## 7.  Supuestos de los datos

>*Instrucciones:* Para aplicar los algoritmos, muchas veces los datos requieren cumplir ciertas condiciones. En caso de ser as√≠, listarlos. 

- Supuesto 1
- Supuesto 2
--- 
## 8. Ejemplos de Aplicaci√≥n

> *Instrucciones:* Presentar uno o m√°s ejemplos de problemas reales en los que se haya utilizado el algoritmo con √©xito. Pueden incluirse ejemplos t√≠picos de aplicaci√≥n del algoritmo.

- Ejemplo 1
- Ejemplo 2
---
## 9. Recursos Adicionales

> *Instrucciones:* Proporcionar enlaces a art√≠culos, libros, cursos o tutoriales que permitan profundizar en el estudio del algoritmo.

- [enlace 1](https://www.ibm.com/topics/support-vector-machine#:~:text=A%20support%20vector%20machine%20(SVM,in%20an%20N%2Ddimensional%20space.)
---
## 10. Referencias

1. Basha, Niha Kamal;Bhatia Khan, Surbhi Bhatia Khan, y Abhishek KumarArwa Mashat. _Deep Learning and Its Applications Using Python_. _Urn:Isbn:9781394166466_. John Wiley & Sons, Incorporated, 2023.
2. Elansari, Taoufyq, Mohammed Ouanan, y Hamid Bourray. ¬´A Novel Mathematical Modeling for Deep Multilayer Perceptron Optimization: Architecture Optimization and Activation Functions Selection¬ª. _Statistics, Optimization & Information Computing_ 12, n.o 5 (6 de junio de 2024): 1409-24. [https://doi.org/10.19139/soic-2310-5070-1990](https://doi.org/10.19139/soic-2310-5070-1990).
3. Shanthamallu, Uday Shankar, Andreas Spanias, Khalid Sayood, Kenichi Kanatani, Nasser Kehtarnavaz, Abhishek Sehgal, Shane Parris, y Arian Azarang. _Machine and Deep Learning Algorithms and Applications_, s.¬†f.